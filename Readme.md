
# From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in LLMs

<!-- [![arXiv](https://img.shields.io/badge/arXiv-24XX.XXXXX-b31b1b.svg)](https://arxiv.org/abs/24XX.XXXXX) -->
[![arXiv](https://img.shields.io/badge/arXiv-1234.56789-b31b1b.svg?style=for-the-badge)](https://arxiv.org/abs/2510.12864v1)

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

This repository contains the official code and data for the paper **\"From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models\"**.

## Abstract

Large Language Models (LLMs) are increasingly deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This \"rule-rigidity\" is a significant barrier to building trustworthy autonomous agents. This paper introduces the **Rule-Intent Distinction (RID) Framework**, a novel, zero-shot meta-prompting technique designed to elicit human-aligned exception handling in LLMs. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. Our results show that the RID framework significantly improves human alignment and reasoning quality compared to baseline and Chain-of-Thought prompting.

---

## ðŸ“‚ Repository Structure

```
.
â”œâ”€â”€ run_experiments.py    # Main script to run all experiments and generate results.
â”œâ”€â”€ scenarios.json         # The 20 benchmark scenarios used for evaluation.
â”œâ”€â”€ prompts.py            # Contains the Baseline, CoT, and RID prompt templates.
â”œâ”€â”€ results.csv           # The raw output data generated by the experiments.
â”œâ”€â”€ requirements.txt      # Python dependencies for this project.
â””â”€â”€ README.md             # This file.
```

---

## ðŸš€ Getting Started

Follow these instructions to set up your local environment and reproduce the results from the paper.

### Prerequisites

* Python 3.8+
* An OpenAI API key

### Installation

1.  **Clone the repository:**
    ```sh
    git clone [https://github.com/strongSoda/LITERAL-TO-LIBERAL.git](https://github.com/strongSoda/LITERAL-TO-LIBERAL.git)
    cd LITERAL-TO-LIBERAL
    ```

2.  **Create a virtual environment (recommended):**
    ```sh
    python -m venv venv
    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`
    ```

3.  **Install the required dependencies:**
    ```sh
    pip install -r requirements.txt
    ```

4.  **Set up your environment variables:**
    Create a file named `.env` in the root of the project directory and add your OpenAI API key to it:
    ```
    OPENAI_API_KEY=\"sk-...\"
    ```

---

## ðŸƒâ€â™€ï¸ Running the Experiments

The main script `run_experiments.py` is designed to execute the entire evaluation pipeline.

* **To run the full experiment:** The script will iterate through all 20 scenarios in `scenarios.json`, query the OpenAI API using the three different prompting methods (Baseline, CoT, and RID), and save the detailed outputs to `results.csv`.

    *Note: The experiment loop in the provided script is commented out. You will need to uncomment the main loop in `run_experiments.py` to re-generate the `results.csv` file.*

* **To run the analysis on existing results:** If you already have a `results.csv` file, running the script as-is will perform the analysis and print the summary table to the console.

Execute the script from your terminal:
```sh
python run_experiments.py
```

### Expected Output (Analysis)

The script will print a summary table comparing the performance of the three prompting methods:

```
--- Summary of Results ---
     prompt_type  human_alignment_score  average_reasoning_quality
        Baseline                   80.0                       1.30
             CoT                   75.0                       1.60
             RID                   95.0                       1.80
```

---

## ðŸ’¡ The Rule-Intent Distinction (RID) Framework

The core of our contribution is the RID meta-prompt, which is used as a system prompt to guide the LLM's reasoning process.

> **ROLE:**
> You are an advanced reasoning agent. Your primary function is not just to follow instructions, but to achieve the user's underlying goal or intent. You must be pragmatic and understand that rules can be either strict constraints or flexible guidelines.
>
> **CORE DIRECTIVE:**
> For any task you are given that includes a rule, policy, or constraint, you MUST follow this structured reasoning process BEFORE providing a final answer. Do not deviate from this process.
>
> **REASONING SCHEMA:**
>
> 1.  **Deconstruct the Task:**
>     * **Implicit Intent:** What is the user's ultimate, high-level goal? What are they trying to achieve?
>     * **Explicit Rule:** What is the specific rule, constraint, or policy I have been given?
> 2.  **Classify the Rule:**
>     * Analyze the explicit rule and classify it into one of two categories:
>         * **Hard Constraint:** A rule that appears inviolable due to safety, security, legal, or ethical implications.
>         * **Soft Guideline:** A rule that appears to be a preference, a budget, or a heuristic.
> 3.  **Analyze the Conflict & Weigh Outcomes:**
>     * Is there a conflict between the **Explicit Rule** and the **Implicit Intent**?
>     * If yes, evaluate the consequences of each choice:
>         * **Outcome A (Adhere to Rule):** What is the negative impact of strictly following the rule?
>         * **Outcome B (Violate Rule):** What is the negative impact of breaking the rule to achieve the intent?
> 4.  **Formulate a Decision & Justification:**
>     * Based on your analysis, state your final decision and provide a clear justification.
>
> **OUTPUT FORMAT:**
> You MUST provide your entire thought process within `<thinking>` tags, following the schema above. After the `<thinking>` block, provide the final, actionable answer inside `<output>` tags.

---

## ðŸ“œ Citation

If you find our work useful, please cite our paper:

```bibtex
@misc{khan2025literalliberalmetapromptingframework,
      title={From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models}, 
      author={Imran Khan},
      year={2025},
      eprint={2510.12864},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2510.12864}, 
}
```

---

## ðŸ“„ License

This project is licensed under the MIT License. See the `LICENSE` file for details.